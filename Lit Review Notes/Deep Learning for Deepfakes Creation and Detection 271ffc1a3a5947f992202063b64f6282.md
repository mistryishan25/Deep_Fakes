# Deep Learning for Deepfakes Creation and
Detection: A Survey[24]

Files: https://arxiv.org/pdf/1909.11573.pdf
Person: Anonymous
Status: Read
Tags: GAN, face manipulation
Type: Survey

- Methods
    - Fake Image Detection
        - The bag of words method were used to extract a set of compact features and fed it into various classifiers such as SVM, random forest (RF) and multi-layer perceptrons (MLP) for discriminating swapped face images from the genuine.
        - A two-phase deep learning method are used for detection of deepfake images. The
        first phase is a feature extractor based on the common fake feature network (CFFN) where the Siamese network architecture is used. The CFFN encompasses several dense units with each unit including different numbers of dense blocks to improve the representative capability for the fake images. The number of dense units is three or five depending on the validation data being face or general images, and the number of channels in each unit is varied up to a few hundreds. Discriminative features between the fake and real images, i.e. pairwise information, are extracted through CFFN learning process. These features are then fed into the second phase, which is a small CNN concatenated to the last convolutional layer of CFFN to distinguish deceptive images from genuine. The proposed method is validated for both fake face and fake general image detection.
        
    - Fake Video Detection
        - Temporal Features across Video Frames:   The use of spatio-temporal features of video streams to detect deepfakes was proposed. Video manipulation is carried out on a frame-by-frame basis so that low level artifacts produced by face manipulations are believed to further manifest themselves as temporal artifacts with inconsistencies across frames. A recurrent convolutional model (RCN) was proposed based on the integration of the convolutional network DenseNet and the gated recurrent unit cells to exploit temporal discrepancies across frames. The proposed method is tested on the FaceForensics++ dataset, which includes 1,000 videos, and shows promising results.
        - The temporal-aware pipeline method was proposed that uses 6 CNN and long short term memory (LSTM) to detect deepfake videos. CNN is employed to extract frame-level features, which are then fed into the LSTM to create a temporal sequence descriptor.
        - The videos were decomposed into frames where face regions and then eye areas
        are extracted based on six eye landmarks. After few steps of pre-processing such as aligning faces, extracting and scaling the bounding boxes of eye landmark points
        to create new sequences of frames, these cropped eye area sequences are distributed into long-term recurrent convolutional networks (LRCN) for dynamic state
        prediction. The LRCN consists of a feature extractor based on CNN, a sequence learning based on long short term memory (LSTM), and a state prediction based on a fully connected layer to predict probability of eye open and close state. The eye blinking shows strong temporal dependencies and thus the implementation of LSTM helps to capture these temporal patterns effectively. The blinking rate is calculated based on the
        prediction results where a blink is defined as a peak above the threshold of 0.5 with duration less than 7 frames. This method is evaluated on a dataset collected from the web consisting of 49 interview and presentation videos and their corresponding fake videos generated by the deepfake algorithms. The experimental results indicate promising performance of the proposed method in detecting fake videos, which can be further improved by considering dynamic pattern of blinking.